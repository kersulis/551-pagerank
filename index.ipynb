{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS 453/551\n",
    "# Google's PageRank algorithm\n",
    "\n",
    "Pagerank, Google's original killer app, is based on SVD. Let's see how it works.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin\n",
    "\n",
    "Run the following cell to install and load the packages we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:43:51.304830",
     "start_time": "2016-09-21T21:43:50.523616"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from surfer import *\n",
    "from surfer_demo import *\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a graph by crawling the internet. **If you don't have a few minutes or so to let `surfer` run, the cell with comment \"for reading from file\" will load a stored version of the graph instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'http://eecs.umich.edu'\n",
    "n = 500 # number of links (nodes in graph)\n",
    "links, G = surfer(root, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:00:43.024754",
     "start_time": "2016-09-21T20:58:49.535542"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for writing to file\n",
    "nx.write_gml(G, \"eecs.gml\")\n",
    "\n",
    "links_file = open(\"links.txt\", \"w\")\n",
    "for l in links:\n",
    "    links_file.write(\"%s\\n\" % l)\n",
    "\n",
    "links_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:43:55.133352",
     "start_time": "2016-09-21T21:43:54.805156"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reading from file\n",
    "G = nx.read_gml(\"eecs.gml\")\n",
    "\n",
    "f = open('links.txt')\n",
    "links = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:43:56.585966",
     "start_time": "2016-09-21T21:43:56.570338"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = nx.adjacency_matrix(G, links).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter `spy(A)` to see the connectivity pattern of the adjacency matrix.\n",
    "\n",
    "Note that `A` is an n-by-n matrix with `A[i,j] = 1` if site `i` is linked to node `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:01:09.060386",
     "start_time": "2016-09-21T21:01:07.560199"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = figure(figsize=(6,6))\n",
    "spy(A, markersize=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple intuitive ways to rank the web pages.\n",
    "\n",
    "1. **Rank by number of incoming links (in degree):** `sum(A,0)` produces a row vector by summing together the rows of the matrix (computing a row sum); each entry of this vector is the number of other sites that links to the corresponding site).\n",
    "2. **Rank by number of outgoing links (out degree):** `sum(A,1)` produces a column vector; each entry of this vector is the number of sites linked to by the corresponding node.\n",
    "\n",
    "When we sum $A$ by rows or columns, we don't automatically see the links. The graph package we're using makes it easy to see the links along with in or out degree. Run the following cell to see the ten sites with the most incoming links. You can do the same for out degree by replacing `G.in_degree_iter` with `G.out_degree_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:01:20.995276",
     "start_time": "2016-09-21T21:01:20.984034"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display links sorted by in degree:\n",
    "din_sort = sorted(G.in_degree_iter(), key=lambda i: i[1], reverse=True)\n",
    "din_sort[:10] # top 10 sites by in degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which site is linked to by the most other sites? Which site links to the most other sites?**\n",
    "\n",
    "Do the numbers make sense given the total number of sites (`len(G.nodes()`) and links between them (`len(G.edges()`)?\n",
    "\n",
    "Which of these rankings seems better? How many sites have no outgoing links, and how should they be ranked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to rank pages\n",
    "The ranking approaches just discussed are easily exploited.\n",
    "\n",
    "* With in degree ranking, someone might set up a thousand pages that link to one page, thereby inflating its rank.\n",
    "* With out degree ranking, someone might include a mountain of links on every page they make to increase its rank.\n",
    "\n",
    "We know there is deeper meaning in our internet graph. Some pages are more important than others. Degree ranking misses a key component of website importance: the number of sites linking to a particular site matters less than the _importance_ of those sites.\n",
    "\n",
    "Our third approach, pagerank, captures this underlying meaning by focusing on probability rather than degree. If one visits site A, what is the probability they will visit site B next? In lecture we discussed a tool that helps answer this question: power iteration. We will view our web browsing graph as a Markov chain and obtain an approximate probability (or transition) matrix. Applying power iteration to this matrix will allow us to obtain a stationary vector, which we will use to rank the web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dangling nodes\n",
    "Nodes that have an incoming link but no outgoing link are referred to as *dangling nodes*. Power iteration won't help us with such nodes present (why?). The adjacency matrix produced by `surfer()` will only have a 1 in the `[i,i]` entry if a page explicitly links to itself, so our web adjacency matrix likely has dangling nodes. We can \"fix\" this by forcing every page to be connected to itself.\n",
    "\n",
    "See how this is implemented in the following function and verify that it matches what we spoke about in lecture. Then **run the cell** to add the function to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:05.617460",
     "start_time": "2016-09-21T21:44:05.613459"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_dangling_nodes(A):\n",
    "    \"\"\"\n",
    "    Return input matrix A (square) with all diagonal elements set to 1.\n",
    "    \"\"\"\n",
    "    return tril(A, -1) + triu(A, 1) + eye(size(A, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing\n",
    "Our goal is to have a matrix where each element $H[i,j]$ is the probability of transitioning from node $i$ to node $j$. Let $G$ be the adjacency matrix with dangling nodes fixed. Then $H$ is generated from $G$ by normalizing the rows to have sum 1.\n",
    "\n",
    "See how this is implemented in the following function. **Run the cell** to add the function to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:06.461011",
     "start_time": "2016-09-21T21:44:06.451013"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_rows(G):\n",
    "    \"\"\"\n",
    "    Return input matrix A normalized so each row\n",
    "    represents probability of leaving node\n",
    "    (each row of returned matrix sums to one)\n",
    "    \"\"\"\n",
    "    out_degree = sum(G, 1)\n",
    "    H = copy(G)\n",
    "    for i in range(len(out_degree)):\n",
    "        if out_degree[i] > 0:\n",
    "            H[i,:] = G[i,:]/out_degree[i]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating 0 elements\n",
    "We're almost ready to apply power iteration, but first we need to address the zeros in our probability matrix. The pagerank vector is defined as the leading eigenvector of the matrix\n",
    "\n",
    "$$\n",
    "\\alpha H + (1-\\alpha)\\mathbf{1}/n\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1}$ is the matrix of all ones and $ 0 < \\alpha < 1$.  The pagerank vector is the unique eigenvector having all positive entries associated with the eigenvalue 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:07.756256",
     "start_time": "2016-09-21T21:44:07.752252"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_zeros(H, alpha):\n",
    "    return alpha*H + (1 - alpha)*ones(shape(H))/size(H,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: adjacency to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:08.661040",
     "start_time": "2016-09-21T21:44:08.642978"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H = normalize_rows(fix_dangling_nodes(A))\n",
    "alpha = 0.85\n",
    "Ha = remove_zeros(H, alpha).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we took the transpose (by adding `.T`), so now $H[i,j]$ is the probability of transitioning from node $j$ to node $i$. You should be comfortable with manipulations like this -- we're only doing it so we can work with right Eigenvectors rather than left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power iteration\n",
    "We're going to do what Google first did: use power iteration to compute the leading eigenvector of $H_\\alpha$, and use this vector to rank our web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:12.234106",
     "start_time": "2016-09-21T21:44:12.219204"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def power_iteration(Ha, steps):\n",
    "    \"\"\"\n",
    "    Beginning with random starting vector, perform specified\n",
    "    number of power iteration steps. Return final vector.\n",
    "    \"\"\"\n",
    "    u = rand(size(Ha, 1))\n",
    "    u = u/norm(u)\n",
    "\n",
    "    for idx in range(steps):\n",
    "        u = dot(Ha, u)\n",
    "        u = u/norm(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:13.023684",
     "start_time": "2016-09-21T21:44:12.982515"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = power_iteration(Ha, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing rankings\n",
    "Run the following cell to create and show a table containing all out in degree, out degree, and pagerank data. Click on a column heading to sort by that column (double-click to sort in descending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:52.063045",
     "start_time": "2016-09-21T21:44:51.873079"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "din_vec = [G.in_degree()[l] for l in links]\n",
    "dout_vec = [G.out_degree()[l] for l in links]\n",
    "\n",
    "df = pd.DataFrame({ 'in_degree' : din_vec,\n",
    "                    'out_degree' : dout_vec,\n",
    "                    'pagerank' : u }, index=links)\n",
    "\n",
    "qgrid.show_grid(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What URL has the second highest pagerank? Does it make sense or is that a bug of `surfer()`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why pagerank wins\n",
    "It may not be clear that pagerank is better than degree sorting. After all, sites like umich.edu and eecs.umich.edu rise to the top in all three rankings. To see why pagerank is superior, consider the site http://eecs.umich.edu/cse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:44:29.612094",
     "start_time": "2016-09-21T21:44:29.606796"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc['http://eecs.umich.edu/cse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our graph, this site has an in degree of 1, an out degree of 69, and a pagerank value of 0.002386. It is near the bottom of the list on all three rankings.\n",
    "\n",
    "Now suppose http://umich.edu and http://regents.umich.edu linked to http://eecs.umich.edu/cse. This would indicate that http://eecs.umich.edu/cse is a very important site. Would our rankings capture that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:46:23.329988",
     "start_time": "2016-09-21T21:46:23.097439"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add our hypothetical graph edges:\n",
    "G.add_edge('http://umich.edu','http://eecs.umich.edu/cse')\n",
    "G.add_edge('http://regents.umich.edu','http://eecs.umich.edu/cse')\n",
    "\n",
    "# recompute adjacency matrix:\n",
    "A2 = nx.adjacency_matrix(G, links).todense()\n",
    "\n",
    "# compute transition matrix Ha:\n",
    "H2 = normalize_rows(fix_dangling_nodes(A2))\n",
    "alpha = 0.85\n",
    "Ha2 = remove_zeros(H2, alpha).T\n",
    "\n",
    "# perform power iteration:\n",
    "u2 = power_iteration(Ha2, 200)\n",
    "\n",
    "# update data table:\n",
    "din_vec = [G.in_degree()[l] for l in links]\n",
    "dout_vec = [G.out_degree()[l] for l in links]\n",
    "\n",
    "df2 = pd.DataFrame({ 'in_degree' : din_vec,\n",
    "                    'out_degree' : dout_vec,\n",
    "                    'pagerank' : u2 }, index=links)\n",
    "\n",
    "qgrid.show_grid(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-21T21:37:58.763343",
     "start_time": "2016-09-21T21:37:58.752246"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.loc['http://eecs.umich.edu/cse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happened?\n",
    "\n",
    "* In degree increased from 1 to 3, a modest increase.\n",
    "* Out degree remained the same.\n",
    "* Pagerank value increased from 0.002386 to 0.19197, a significant boost!\n",
    "\n",
    "Despite being linked to by two of the most important pages, http://eecs.umich.edu/cse did not reach the top of in degree or out degree rankings. But its pagerank boost was huge: its new value of 0.191969 makes it the 5th highest site by pagerank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "Check out the original [Google PageRank patent](http://www.google.com/patents?id=cJUIAAAAEBAJ&printsec=description&zoom=4#v=onepage&q&f=false).\n",
    "\n",
    "Check out page 8, line 35 (right column) for the exact $H\\alpha$ matrix we used and the description of the algorithm. They claim that $\\alpha = 0.85$ (they use $1-\\alpha$) is what \"works well\". They never mention the word \"eigenvector\" but clearly that is what is being done. Note the description of the power method in the flowchart on page 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "0a7b39f98f244a93894cea55715507df": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    },
    "f5d522a11ec945d7b0f188b43c8874be": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
